diff --git a/analysis.py b/analysis.py
index 36da890..3848f1c 100644
--- a/analysis.py
+++ b/analysis.py
@@ -5,7 +5,7 @@ import pandas as pd
 import matplotlib.pyplot as plt
 
 
-MODEL_NAME = ""
+MODEL_NAME = "models/63.23-acc-loss-2.52.model"
 
 CLIP = True  # if your model was trained with np.clip to clip  values
 CLIP_VAL = 10  # if above, what was the value +/-
@@ -13,7 +13,7 @@ CLIP_VAL = 10  # if above, what was the value +/-
 model = tf.keras.models.load_model(MODEL_NAME)
 
 VALDIR = 'validation_data'
-ACTIONS = ['left','none','right']
+ACTIONS = ['left', 'none', 'right']
 PRED_BATCH = 32
 
 
@@ -24,18 +24,19 @@ def get_val_data(valdir, action, batch_size):
 
     action_dir = os.path.join(valdir, action)
     for session_file in os.listdir(action_dir):
-        filepath = os.path.join(action_dir,session_file)
+        filepath = os.path.join(action_dir, session_file)
         if CLIP:
             data = np.clip(np.load(filepath), -CLIP_VAL, CLIP_VAL) / CLIP_VAL
         else:
-            data = np.load(filepath) 
+            data = np.load(filepath)
 
-        preds = model.predict([data.reshape(-1, 16, 60)], batch_size=batch_size)
+        preds = model.predict([data.reshape(-1, 16, 60)],
+                              batch_size=batch_size)
 
         for pred in preds:
             argmax = np.argmax(pred)
             argmax_dict[argmax] += 1
-            for idx,value in enumerate(pred):
+            for idx, value in enumerate(pred):
                 raw_pred_dict[idx] += value
 
     argmax_pct_dict = {}
@@ -66,18 +67,23 @@ def make_conf_mat(left, none, right):
     print("__________")
     print(action_dict)
     for idx, i in enumerate(action_dict):
-        print('tf',i)
+        print('tf', i)
         for idx2, ii in enumerate(action_dict[i]):
             print(i, ii)
             print(action_dict[i][ii])
-            ax.text(idx, idx2, f"{round(float(action_dict[i][ii]),2)}", va='center', ha='center')
+            ax.text(
+                idx, idx2, f"{round(float(action_dict[i][ii]),2)}", va='center', ha='center')
     plt.title("Action Thought")
     plt.ylabel("Predicted Action")
     plt.show()
 
 
-left_argmax_dict, left_raw_pred_dict, left_argmax_pct_dict = get_val_data(VALDIR, "left", PRED_BATCH)
-none_argmax_dict, none_raw_pred_dict, none_argmax_pct_dict = get_val_data(VALDIR, "none", PRED_BATCH)
-right_argmax_dict, right_raw_pred_dict, right_argmax_pct_dict = get_val_data(VALDIR, "right", PRED_BATCH)
+left_argmax_dict, left_raw_pred_dict, left_argmax_pct_dict = get_val_data(
+    VALDIR, "left", PRED_BATCH)
+none_argmax_dict, none_raw_pred_dict, none_argmax_pct_dict = get_val_data(
+    VALDIR, "none", PRED_BATCH)
+right_argmax_dict, right_raw_pred_dict, right_argmax_pct_dict = get_val_data(
+    VALDIR, "right", PRED_BATCH)
 
-make_conf_mat(left_argmax_pct_dict, none_argmax_pct_dict, right_argmax_pct_dict)
+make_conf_mat(left_argmax_pct_dict, none_argmax_pct_dict,
+              right_argmax_pct_dict)
diff --git a/training.py b/training.py
index d3838c3..7d1fb71 100644
--- a/training.py
+++ b/training.py
@@ -4,113 +4,150 @@ from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
 from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization
 import os
-import random
+import wandb
+from wandb.keras import WandbMetricsLogger
 import time
 
-
 ACTIONS = ["left", "right", "none"]
 reshape = (-1, 16, 60)
 
-def create_data(starting_dir="data"):
-    training_data = {}
-    for action in ACTIONS:
-        if action not in training_data:
-            training_data[action] = []
-
-        data_dir = os.path.join(starting_dir,action)
-        for item in os.listdir(data_dir):
-            #print(action, item)
-            data = np.load(os.path.join(data_dir, item))
-            for item in data:
-                training_data[action].append(item)
-
-    lengths = [len(training_data[action]) for action in ACTIONS]
-    print(lengths)
-
-    for action in ACTIONS:
-        np.random.shuffle(training_data[action])  # note that regular shuffle is GOOF af
-        training_data[action] = training_data[action][:min(lengths)]
-
-    lengths = [len(training_data[action]) for action in ACTIONS]
-    print(lengths)
-    # creating X, y 
-    combined_data = []
-    for action in ACTIONS:
-        for data in training_data[action]:
-
-            if action == "left":
-                combined_data.append([data, [1, 0, 0]])
-
-            elif action == "right":
-                #np.append(combined_data, np.array([data, [1, 0]]))
-                combined_data.append([data, [0, 0, 1]])
-
-            elif action == "none":
-                combined_data.append([data, [0, 1, 0]])
-
-    np.random.shuffle(combined_data)
-    print("length:",len(combined_data))
-    return combined_data
-
-
-print("creating training data")
-traindata = create_data(starting_dir="data")
-train_X = []
-train_y = []
-for X, y in traindata:
-    train_X.append(X)
-    train_y.append(y)
 
-print("creating testing data")
-testdata = create_data(starting_dir="validation_data")
-test_X = []
-test_y = []
-for X, y in testdata:
-    test_X.append(X)
-    test_y.append(y)
+def main():
+    wandb.init(
+        project="Directions_BCI"
+    )
+    # hpyerparameters
+    epochs = 1
+    batch_size = 32
+    out_layers = [512, 128, 128]
+    out_layers_activation = "relu"
+    conv_activation = "relu"
+    noise = 0
+
+    if wandb.config is not None:
+        out_layers = [wandb.config["a"], wandb.config["b"], wandb.config["c"]]
+
+    def create_data(starting_dir="data"):
+        training_data = {}
+        for action in ACTIONS:
+            if action not in training_data:
+                training_data[action] = []
+
+            data_dir = os.path.join(starting_dir, action)
+            for item in os.listdir(data_dir):
+                # print(action, item)
+                data = np.load(os.path.join(data_dir, item))
+                for item in data:
+                    training_data[action].append(
+                        item + np.random.normal(0, noise, item.shape))
+
+        lengths = [len(training_data[action]) for action in ACTIONS]
+        print(lengths)
+
+        for action in ACTIONS:
+            # note that regular shuffle is GOOF af
+            np.random.shuffle(training_data[action])
+            training_data[action] = training_data[action][:min(lengths)]
+
+        lengths = [len(training_data[action]) for action in ACTIONS]
+        print(lengths)
+        # creating X, y
+        combined_data = []
+        for action in ACTIONS:
+            for data in training_data[action]:
+
+                if action == "left":
+                    combined_data.append([data, [1, 0, 0]])
+
+                elif action == "right":
+                    combined_data.append([data, [0, 0, 1]])
+
+                elif action == "none":
+                    combined_data.append([data, [0, 1, 0]])
+
+        np.random.shuffle(combined_data)
+        print("length:", len(combined_data))
+        return combined_data
+
+    print("creating training data")
+    traindata = create_data(starting_dir="data")
+    train_X = []
+    train_y = []
+    for X, y in traindata:
+        train_X.append(X)
+        train_y.append(y)
+
+    print("creating testing data")
+    testdata = create_data(starting_dir="validation_data")
+    test_X = []
+    test_y = []
+    for X, y in testdata:
+        test_X.append(X)
+        test_y.append(y)
+
+    print(len(train_X))
+    print(len(test_X))
+
+    print(np.array(train_X).shape)
+    train_X = np.array(train_X).reshape(reshape)
+    test_X = np.array(test_X).reshape(reshape)
+
+    train_y = np.array(train_y)
+    test_y = np.array(test_y)
+
+    model = Sequential()
+
+    model.add(Conv1D(64, (3), input_shape=train_X.shape[1:]))
+    model.add(Activation(conv_activation))
+
+    model.add(Conv1D(128, (2)))
+    model.add(Activation(conv_activation))
+
+    model.add(Conv1D(128, (2)))
+    model.add(Activation(conv_activation))
+
+    model.add(Conv1D(64, (2)))
+    model.add(Activation(conv_activation))
+    model.add(MaxPooling1D(pool_size=(2)))
+
+    model.add(Conv1D(64, (2)))
+    model.add(Activation(conv_activation))
+    model.add(MaxPooling1D(pool_size=(2)))
+
+    model.add(Flatten())
+
+    for layer in out_layers:
+        model.add(Dense(layer))
+        model.add(Activation(out_layers_activation))
+
+    model.add(Dense(3))
+    model.add(Activation('softmax'))
 
-print(len(train_X))
-print(len(test_X))
+    model.compile(loss='categorical_crossentropy',
+                  optimizer='adam',
+                  metrics=['accuracy'])
 
+    model.fit(train_X, train_y, batch_size=batch_size,
+              epochs=epochs, validation_data=(test_X, test_y), callbacks=[WandbMetricsLogger()])
+    score = model.evaluate(test_X, test_y, batch_size=batch_size)
+    # print(score)
+    MODEL_NAME = f"new_models/{round(score[1]*100,2)}-acc-64x3-batch-norm-{int(time.time())}-loss-{round(score[0],2)}.model"
+    model.save(MODEL_NAME)
+    print("saved:")
+    print(MODEL_NAME)
 
-print(np.array(train_X).shape)
-train_X = np.array(train_X).reshape(reshape)
-test_X = np.array(test_X).reshape(reshape)
-
-train_y = np.array(train_y)
-test_y = np.array(test_y)
-
-model = Sequential()
-
-model.add(Conv1D(64, (3), input_shape=train_X.shape[1:]))
-model.add(Activation('relu'))
-
-model.add(Conv1D(64, (2)))
-model.add(Activation('relu'))
-model.add(MaxPooling1D(pool_size=(2)))
-
-model.add(Conv1D(64, (2)))
-model.add(Activation('relu'))
-model.add(MaxPooling1D(pool_size=(2)))
-
-model.add(Flatten())
 
-model.add(Dense(512))
+sweep_configuration = {
+    "method": "bayes",
+    "metric": {"goal": "maximize", "name": "val_accuracy"},
+    "parameters": {
+        "a": {"values": [8, 16, 32, 64, 128, 256, 512, 1024]},
+        "b": {"values": [8, 16, 32, 64, 128, 256, 512, 1024]},
+        "c": {"values": [8, 16, 32, 64, 128, 256, 512, 1024]},
+    },
+}
 
-model.add(Dense(3))
-model.add(Activation('softmax'))
 
-model.compile(loss='categorical_crossentropy',
-              optimizer='adam',
-              metrics=['accuracy'])
+sweep_id = wandb.sweep(sweep=sweep_configuration, project="Directions_BCI")
 
-epochs = 10
-batch_size = 32
-for epoch in range(epochs):
-    model.fit(train_X, train_y, batch_size=batch_size, epochs=1, validation_data=(test_X, test_y))
-    score = model.evaluate(test_X, test_y, batch_size=batch_size)
-    #print(score)
-    MODEL_NAME = f"new_models/{round(score[1]*100,2)}-acc-64x3-batch-norm-{epoch}epoch-{int(time.time())}-loss-{round(score[0],2)}.model"
-    model.save(MODEL_NAME)
-print("saved:")
-print(MODEL_NAME)
+wandb.agent(sweep_id, function=main, count=10)
